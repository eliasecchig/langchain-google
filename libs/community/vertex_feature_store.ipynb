{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GenAI RAG application with Feature Store and BigQuery\n",
    "\n",
    "## Overview\n",
    "This notebook guides you through building a low-latency vector search system for your GenAI application using Vertex AI Feature Store. We'll leverage the [Vertex Feature Store Langchain integration]([link to integration]) to streamline this process.\n",
    "\n",
    "Feature Store seamlessly integrates with BigQuery, providing a unified data storage and flexible vector search options:\n",
    "\n",
    "- **BigQuery Vector Search**: Ideal for batch retrieval and prototyping, as it requires no infrastructure setup.\n",
    "- **Feature Store Online Store**: Enables low-latency retrieval with manual or scheduled data sync. Perfect for production-ready user-facing GenAI applications.\n",
    "\n",
    "![Image notebook journey](diagram_journey.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain-google-vertexai pypdf==4.2.0 langchain pyarrow==16.0.0 db-dtypes==1.2.0 scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe7OuYuGkLKF"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9Gx2SAZkLKF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxH62gFHCFPj"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_community.feature_store.bigquery import BigQueryVectorStore\n",
    "from langchain_google_community.feature_store.featurestore import VertexFSVectorStore\n",
    "from langchain_google_community.feature_store.local_store import BigQueryInMemoryVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"cloud-llm-preview2\"\n",
    "DATASET = \"vertex_documentation\"\n",
    "TABLE = \"mytest99\"\n",
    "REGION = \"europe-west4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDSM2ZQa8eBs"
   },
   "source": [
    "# Add documents to `BigQueryVectorStore`\n",
    "\n",
    "This step ingests and parse PDF documents, split them, generate embeddings and add the embeddings to the vector store. The document corpus used as dataset is a collection of owners car manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIML1Ttj5nTk"
   },
   "source": [
    "**Summary steps**\n",
    "- Create text embeddings: LangChain `VertexAIEmbeddings`\n",
    "- Ingest PDF files: LangChain `PyPDFLoader`\n",
    "- Chunk documents: LangChain `TextSplitter`\n",
    "- Create Vector Store: LangChain  `VertexAIFeatureStore` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3hRmLiY2O0w"
   },
   "source": [
    "### Create the VertexAI Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tRYGd_Mv_ix"
   },
   "outputs": [],
   "source": [
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKM_QIU0WPc3"
   },
   "source": [
    "### Ingest PDF file\n",
    "\n",
    "The document is hosted on Cloud Storage bucket (at `gs://github-repo/generative-ai/sample-apps/fixmycar/cymbal-starlight-2024.pdf`) and LangChain provides a convenient document loader [`PyPDFLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf/) to load documents from pdfs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNEClQQjjRcc",
    "outputId": "de918892-4fae-4c5b-cd4d-915f87d11a06"
   },
   "outputs": [],
   "source": [
    "GCS_BUCKET_DOCS = (\n",
    "    \"github-repo/generative-ai/sample-apps/fixmycar\"  # @param {type: \"string\"}\n",
    ")\n",
    "\n",
    "# Copy the file to the current path\n",
    "!gsutil cp \"gs://$GCS_BUCKET_DOCS/*.pdf\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kYw66EohYza",
    "outputId": "098e5ce1-cc21-4be4-c4c0-f2f1e203e5d6"
   },
   "outputs": [],
   "source": [
    "# Ingest PDF files\n",
    "loader = PyPDFLoader(\"cymbal-starlight-2024.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Add document name and source to the metadata\n",
    "for document in documents:\n",
    "    doc_md = document.metadata\n",
    "    document_name = doc_md[\"source\"].split(\"/\")[-1]\n",
    "    # derive doc source from Document loader\n",
    "    doc_source_prefix = \"/\".join(GCS_BUCKET_DOCS.split(\"/\")[:3])\n",
    "    doc_source_suffix = \"/\".join(doc_md[\"source\"].split(\"/\")[4:-1])\n",
    "    source = f\"{doc_source_prefix}/{doc_source_suffix}\"\n",
    "    document.metadata = {\"source\": source, \"document_name\": document_name}\n",
    "\n",
    "print(f\"# of documents loaded (pre-chunking) = {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8J7hQkVmkB1"
   },
   "source": [
    "Verify document metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jn3A13Nfmihf",
    "outputId": "3d56d945-7458-4d03-f6ce-09b9f687bada"
   },
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZA9HDA0mnWW"
   },
   "source": [
    "## Chunk documents - TextSplitter\n",
    "\n",
    "Split the documents to smaller chunks. When splitting the document, ensure a few chunks can fit within the context length of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2QuilhMmxgA",
    "outputId": "1acae5ed-82ea-4d30-85b3-804976fbcb54"
   },
   "outputs": [],
   "source": [
    "# split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add chunk number to metadata\n",
    "for idx, split in enumerate(doc_splits):\n",
    "    split.metadata[\"chunk\"] = idx\n",
    "\n",
    "print(f\"# of documents = {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fv91BaKCmxP0",
    "outputId": "fce7f8af-94e7-44d6-e554-84a682dc41d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_splits[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QC-w_t3m9Fe"
   },
   "source": [
    "## Configure `BigQueryVectorStore` as Vector Store\n",
    "\n",
    "You are now ready to use BigQuery Vector Store!\n",
    "\n",
    "You can initialize the class by providing:\n",
    "- `project_id`\n",
    "- `location`\n",
    "- `dataset_name`\n",
    "- `table_name`\n",
    "\n",
    "The table will be used to store embeddings and metadata. You can also point to an existing table. \n",
    "\n",
    "The class will use [BigQuery Vector Search](https://cloud.google.com/bigquery/docs/vector-search-intro) to perform vector search.\n",
    "\n",
    "See [here](TODO) for the full list of parameters of the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    embedding=embedding_model,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents to the store\n",
    "\n",
    "Note: If you have precomputed embeddings, you can add text, embeddings and potential metadata using the method `add_texts_with_embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_store.add_documents(doc_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the BigQueryVectorSearch with similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_store.similarity_search(\n",
    "    \"What should I do when I call the emergency roadside assistance?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Get a langchain retriever\n",
    "The retriever will be used in a Langchain Chain to find the most similar documents for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_retriever = bq_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose a Langchain Chain\n",
    "\n",
    "We are going to use the [`RetrievalQA` chain](https://python.langchain.com/docs/modules/chains/popular/vector_db_qa)\n",
    "There are several different chain types available, listed [here](https://docs.langchain.com/docs/components/chains/index_related_chains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "# Set high verbosity\n",
    "set_debug(True)\n",
    "\n",
    "llm = VertexAI(model_name=\"gemini-pro\")\n",
    "\n",
    "search_query = \"What should I do when call the emergency roadside assistance?\"  # @param {type:\"string\"}\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=langchain_retriever\n",
    ")\n",
    "response = retrieval_qa.invoke(search_query)\n",
    "print(\"\\n################ Final Answer ################\\n\")\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low latency Vector Search with FeatureStore\n",
    "\n",
    "We are now ready to perform low latency serving with Feature Store! \n",
    "\n",
    "To do that, you can simply use the method `.get_vertex_fs_vector_store()`, to get a `VertexFSVectorStore` object\n",
    "\n",
    "See the [function definition](TODO) for all the parameters you can use.\n",
    "\n",
    "Note: Any method we run earlier can be equivalently called on both `BigQueryVectorStore` and `VertexFSVectorStore`. For instance it is possible to add new documents to an instance of `VertexFSVectorStore` as both stores share the same underlying BQ source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_fs = bq_store.get_vertex_fs_vector_store() # pass optional parameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively you can also init the VertexFSVectorStore class directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_fs = VertexFSVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    embedding=embedding_model,\n",
    "    # pass optional parameters here\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kick off a synchronization process\n",
    "\n",
    "We use the `sync` method to synchronize the data from BigQuery to the Feature Online Store, to achieve low latency serving.\n",
    "\n",
    "When in a production environment, you can also use `cron_schedule` to setup an automatic scheduled synchronization. \n",
    "\n",
    "The synchronization process will take around ~20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# force sync\n",
    "vertex_fs.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the synchronization process from GCP Console: [Vertex AI Feature Store Tab](https://console.cloud.google.com/vertex-ai/feature-store/online-stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serve with Feature Online Store\n",
    "\n",
    "You are now ready to serve with Feature Store!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_retriever = vertex_fs.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = langchain_retriever.invoke(\"Leaks under the vehicle\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = retrieval_qa.invoke(search_query)\n",
    "print(\"\\n################ Final Answer ################\\n\")\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by metadata\n",
    "\n",
    "It is possible to post-filter results by metadata by passing the filter parameter to any search method\n",
    "\n",
    "VertexFSVectorStore also support metadata filter while performing search, for this to work:\n",
    "- the `filter_columns` parameter must be passed to `VertexFSVectorStore` when the online feature store feature view is created (first time the class is initialised with a given online store name and feature view name).\n",
    "\n",
    "- the `string_filters` parameter must be passed to any search method. Note only string fields are supported at the moment. See [here](https://github.com/googleapis/python-aiplatform/blob/8a4a41afe47aaff2f69a73e5011b34bcba5cd2e9/google/cloud/aiplatform_v1beta1/types/feature_online_store_service.py#L345) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform post search filtering\n",
    "vertex_fs.similarity_search(search_query, filter={'chunk': 56})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch search\n",
    "\n",
    "For some use cases it is necessary to run batch searches (ie. when running a retrieval evaluation).\n",
    "\n",
    "Instead of running a search for each query in a loop we can do that more efficiently by running a batch search.\n",
    "\n",
    "While any of the classes introduced in this notebook can run batch searches, the most efficient way of doing it is by using the `BigQueryVectorStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a bq vector store back\n",
    "bq_vector_store = vertex_fs.get_big_query_vector_store()\n",
    "\n",
    "bq_vector_store.batch_search(\n",
    "    embeddings=None, # can pass embeddings or\n",
    "    queries=[search_query, search_query], # can pass queries\n",
    "    with_scores=True, # return matching scores\n",
    "    with_embeddings=True # return matched embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Bruteforce\n",
    "\n",
    "You can also prototype by using a (local) bruteforce executor. During initialization, data is downloaded from BQ to your memory.\n",
    "\n",
    "You can use it for prototyping when the number of documents is low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_store = BigQueryInMemoryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "# sync the data from BQ\n",
    "memory_store.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_store.similarity_search(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_retriever = memory_store.as_retriever()\n",
    "memory_retriever.invoke(search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Marginal Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_retriever = vertex_fs.as_retriever(search_type=\"mmr\")\n",
    "mmr_retriever.invoke(search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get documents by ID\n",
    "\n",
    "You can also use the function `get_documents` to retrieve a set of documents given a document ID:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_fs.get_documents(ids=[\"6470d610abbd40e2af3c32ede29e09d5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove documents by ID\n",
    "\n",
    "You can also use the function `delete` to remove a set of documents given a document ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_fs.delete(ids=[\"my_id1\", \"my_id2\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
